{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from wordcloud import WordCloud \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Precily_Text_Similarity.csv\")\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the text in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(copydata):\n",
    "    pattern = r'[' + string.punctuation + ']'\n",
    "    copydata['text1']=data['text1'].map(lambda m:re.sub(pattern,\" \",m))\n",
    "    copydata['text2']=data['text2'].map(lambda m:re.sub(pattern,\" \",m))\n",
    "    return copydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(df):\n",
    "    df['text1'] = df['text1'].map(lambda row: row.lower())\n",
    "    df['text2'] = df['text2'].map(lambda row: row.lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(df):\n",
    "    df['text1'] = df['text1'].apply(word_tokenize)\n",
    "    df['text2'] = df['text2'].apply(word_tokenize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on tokenized data\n",
    "def rm_SW(df):\n",
    "    sw=nltk.corpus.stopwords.words('english')\n",
    "    df['text1'] = df['text1'].apply(lambda x: [item for item in x if item not in sw])\n",
    "    df['text2'] = df['text2'].apply(lambda x: [item for item in x if item not in sw])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_digits(df):\n",
    "    df['text1'] = df['text1'].apply(lambda row: re.sub('\\d.*','',row))\n",
    "    df['text1'] = df['text1'].apply(lambda row: re.sub('\\d.*','',row))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ON LEMETIZE DATA\n",
    "def lmtz(df):\n",
    "    lm = WordNetLemmatizer()\n",
    "    df['text1']=df['text1'].apply(lambda row: [lm.lemmatize(word) for word in row])\n",
    "    df['text2']=df['text2'].apply(lambda row: [lm.lemmatize(word) for word in row])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings from pre trained model -> Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer,util\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pairs = df[[\"text1\", \"text2\"]].values\n",
    "print(sentence_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1, embeddings2 = [], []\n",
    "for i in range(len(df)):\n",
    "  new_embedding1 = model.encode(df['text1'][i])\n",
    "  embeddings1.append(new_embedding1)\n",
    "  new_embedding2 = model.encode(df['text2'][i])\n",
    "  embeddings2.append(new_embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = cosine_similarity(embeddings1, embeddings2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
